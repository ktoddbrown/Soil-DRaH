---
title: "Carbon in Peat on EArth through Time"
author: "Vaasuki Marupaka"
date: "Summer 2024"
output: 
  html_document: 
    toc: true
    number_sections: true
    code_folding: show
---

The purpose of this document is to summarize the portions of the Carbon in Peat on EArth through Time (C-PEAT) Database that are relevant to data collections in the SoilDRaH project and walk through the data ingestion.
Here you will find links to documentation from the data provider, links to where you can access the data, data model description, data processing workflows, and visuals for the collection relevant variables.

# What is C-PEAT

The version of the Carbon in Peat on EArth through Time (C-PEAT) Database described here is the public facing version of the PANGAEA repository (https://www.pangaea.de).

> **C-PEAT Database**
> Peatlands played a key role in the global carbon cycle during the Holocene and previous interglacials. High-latitude and tropical peatlands have acted as net long-term atmospheric sinks for carbon dioxide (CO2). However, key uncertainties remain regarding many fundamental patterns and processes. C-PEAT aims to address these uncertainties.

Taken from [https://pastglobalchanges.org/science/wg/former/peat-carbon/intro](https://pastglobalchanges.org/science/wg/former/peat-carbon/intro) (Accessed 17-June-2024)

The annotations table draws heavily from the parameters described in the orginal metadata and also from the point of contact for C-PEAT collection of datasets (Dr. Julie Loisel). 

The C-PEAT database on pangaea is presented in a complex nested format with the primary study level and metadata information presented as a graphical database structure while the layer level information or the data table are presented in regular relational database format. We have worked with merging this complex data structure and collapsing it into one single table format. For example, when you call the function $str$ to find out the structure of the object, the data is presented in a nested list format for the original CPEAT database structure. Our goal is to shoestring this data into one long table along with the annotations. 

# Data processing

```{r setup, echo=TRUE, warning=FALSE, message=FALSE}

library(readr) # read in the csv tables
library(tibble) # use tibbles instead of a data frame
library(plyr) # transform a list into a data frame for the bind
library(dplyr) # work with data tables filters/joins/reframes
library(tidyr) # work with data table pivots
library(ggplot2) # make plots
library(stringr) # extract text from descriptions
library(knitr) # make prettier tables
library(kableExtra) #make tables scrollable
library(pangaear)

source("./R/readCPEAT.R", local = knitr::knit_global())

#locate the data locally
dataDir <- './temp/CPEAT'
#dataAnnotations <- '../data/CPEAT_annotations.csv'
dataAnnotations <- './data/annotations_CPEAT2.csv'
```

```{r, echo=FALSE}
#Change this file to run locally for you
#dataDir  <- '~/Dropbox (UFL)/Research/Datasets/'
```

Read in the data use the `readCPEAT` function.

```{r, warning=FALSE, message=FALSE}
CPEAT.ls <- readCPEAT(dataDir, 
                  annotationFilename = dataAnnotations, 
                  #when first running, switch this to true 
                  verbose = FALSE, 
                  format = 'long',
                  randomSubset = Inf) # 2,640,892 obs for all datasets

CPEAT.original <- readCPEAT(dataDir, 
                  annotationFilename = dataAnnotations, 
                  #when first running, switch this to true 
                  verbose = FALSE, 
                  format = 'original',
                  randomSubset = Inf)

#read_csv(file = 'data/annotations_CPEAT2.csv') |>
#  arrange(header_id, table_name) |>
#  write_csv(file = 'data/annotations_CPEAT2.csv')

CPEAT.ls$long %>%  # 2,640,892 entries, 7 total columns
  slice_head(n=650) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```


## CPEAT Data structures

Each core in CPEAT has it's own doi-identified dataset.
This data set includes a graphical meta-data at the front of a tab separated data table.
See example below:

```{r echo=FALSE}
examplefile <- readLines(list.files(file.path(dataDir), 
                                    full.names = TRUE)[10])
cat(paste0(examplefile, collapse = '\n'))
#Figure out how to show a CPEAT file content here
```

We currently use XX function in the YY package to process this data into a the metadata portion (list-based data) and associated primary data (table-based data).

```{r echo=FALSE, warning=FALSE, message=FALSE}
singleExample <- readCPEAT(dataDir = dataDir, 
          annotationFilename = dataAnnotations,
          format = 'original',
          randomSubset = 1,
          verbose = FALSE)

str(singleExample$original[[1]])
```

We annotate this metadata information a little differently then is typical for the project.

The readCPEAT function performs the task of downloading, processing, and transforming datasets related to the CPEAT (PAGES C-PEAT) project. It interacts with the Pangaea data repository using default functions provided by pangaear package, process their nested structures, and transform them into a more uniform, relational data table format that can be manipulated and analyzed more easily. Hereâ€™s a breakdown of the function, what it's doing, and why it is structured this way:

The function begins by searching the Pangaea repository ($pg\_search$) for datasets labeled with the CPEAT project. It includes metadata about these datasets (e.g., DOIs, citations, parameters and events related information). Each dataset (using function $pg\_data$) is downloaded as a structured list from Pangaea, and the function needs to retain this structure temporarily to parse and process it later.

The parameters list is nested under metadata which includes the metadata; description, method, unit, and any additional information aligned with the data table variables. This metadata is often inconsistently formatted (e.g., mismatch of the the length of columns), so the function cleans and standardizes it by ensuring that parameter descriptions match the data columns. The function creates columns out of data table variables/headers by extracting the id, unit, method information using regular expressions ($str\_extract$).

Firstly, the nested lists are transformed into a single wide table with all the columns. For better handling and data manipulation, the function transforms into a long table format ($pivot\_longer$) along with unique column numbers and row ids grouped by specific doi. The long format helps in data analysis, making it easier to perform operations like filtering, aggregating, and visualizing the data. 

In the final output, the long table consists of the columns: $doi$, $table\_name$, $column\_number$, $row\_number$, $of\_variable$, $is\_type$, $with\_entry$ that resemble the typical annotations template we follow for data harmonization efforts. 

The table names for this database include: primary study information (denoted by . in the long table), metadata, metadata\$events, metadata\$parameters, and data tables. 

The of\_variable column describes the variables linked to the column\_id, such as bulk density, organic carbon, vegetation, other covariates. It's important to maintain the original intent of the data providers when creating these annotations. While the of\_variable entries are highly dependent on the dataset, is\_type should have a more restricted range of entries.

The is\_type entry usually falls into one of the following categories: {value, definition, unit, method, control\_vocabulary}. This entry describes the type of information associated with of\_variable found in with\_entry. Note that we use 'value' as a general term for simplicity, regardless of whether the data is numerical or text. 

The with\_entry column contains either the specific entry associated with the variable and type described or a special notation indicating a reference to the dataset being described. We currently use -- to denote a dataset reference.

# Subseting for soil carbon

There is a lot of data here but not all of it is of interest for all purposes.
In this example we are interested in layer-level soil carbon related measurements and the geolocation.
Note that we are NOT checking any of the quality control flags here which you would want to do if using this for research purposes.

```{r}
CPEAT_soc <- CPEAT.ls$long %>% 
  #identify the variables that we are interested in
  filter(of_variable %in% c(
    "doi",
    "data_doi",
    "download_url",
    "carbon",
    "bulk_density",
    "layer_mid",
    "layer_thickness",
    "latitude",
    "longitude",
    "region",
    "elevation",
    "elevation_end",
    "elevation_start",
    "loss_on_ignition",
    "organic_matter",
    "organic_matter_density",
    "total_carbon",
    "total_organic_carbon",
    "inorganic_carbon"),
    #limit these to tables since some of them are duplicates
    table_name %in% c('.', 'metadata$events', 'data')) %>%
  #only take the values, just incase there are any methods or units in the table
  filter(is_type == 'value', is.na(is_type) | is_type != 'description') %>%
  unique()

CPEAT_soc %>%
  slice_head(n=100) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```

## Make small tables

Often this data is presented in three tables: a site level table with lat/lon and year, a layer level table with the depth and physiochemical information, and a meta table with information on the units and descriptions.
We given an example below of how this might be organized.

```{r}
# site level info
site.df <- CPEAT_soc %>%
  filter(of_variable %in% c(
    #"doi",
    "latitude",
    "longitude",
    "region",
    "elevation",
    "elevation_end",
    "elevation_start")) %>%
  filter(!is.na(with_entry)) %>% 
  unique() %>% 
  pivot_wider(names_from = of_variable,
               values_from = with_entry)

#just renaming the object to maintain uniformity in naming across datasets
layer.df <- CPEAT_soc %>% 
  select(doi_primary = doi, table_name, row_number, of_variable, with_entry) %>%
  filter(!is.na(with_entry)) %>%
  pivot_wider(names_from = of_variable,
    values_from = with_entry,
    values_fn = list(with_entry = ~ paste(., collapse = "; "))) %>% 
  select(-doi) %>% 
  rename(doi = 'doi_primary')  # 98,596 obs of 18 variables

```

## Casting values

Finally make sure that the numbers are actually numbers.

```{r}
# changing the structure of the column variables except doi and other character variables
site.df <- site.df %>% 
  dplyr::mutate(across(c(latitude, longitude, elevation), as.numeric),
                region = factor(region))

layer.df <- layer.df %>%  
  mutate(across(-c(doi, table_name, row_number, data_doi, 
                   download_url, region), as.numeric)) # 98,596 obs of 18 variables

```

# Factor tables  

```{r}
# count of regions for CPEAT layer level data
site.df %>% 
  group_by(region) %>% 
  tally() %>%
  knitr::kable(caption = 'CPEAT layer-level catagorical regions') %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")

layer_site <- site.df %>% 
  mutate(has_lat_long = is.finite(latitude + longitude)) %>% 
  group_by(doi, has_lat_long, region) %>% 
  tally() %>% 
  mutate(location_label = if_else(has_lat_long, "geolocated_layer", "unlocated_layer")) %>% 
  ungroup() # 870 obs of 5 variables

```

## Figures and Report

Below are some general figures that might be of interest if you are evaluating this database for use in a study.

# Plotting Layer histograms!

```{r}
ggplot(CPEAT_soc %>%
         pivot_longer(cols = where(is.numeric), values_drop_na = TRUE)) +
  geom_histogram(aes(x=value)) +
  facet_wrap(~of_variable, scales='free')
```

# Bulk density vs Organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x = bulk_density, y = carbon), alpha = 0.1)
```

# Bulk density vs Total organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x = bulk_density, y = total_organic_carbon), alpha = 0.1)
```

# Geolocations

```{r}
# plotting geolocations with lat and long values 
ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x=longitude, y = latitude), alpha = .1, color = 'darkorange') + 
  labs(x = NULL, y = NULL)

# categorizing the geolocations using region factor
ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x=longitude, y = latitude,
                 fill = region, color = region), size = 1.0) +
  #theme(legend.position = "bottom")  +
  labs(x = NULL, y = NULL)
```

```{r}
install.packages("plotly")
library(plotly)

# Create the interactive ggplot 
plot_cpeat <- ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x = longitude, y = latitude, text = region), 
             alpha = 0.2, size = 0.4, color = 'darkblue') +
  labs(title = 'Geolocation for 870 CPEAT cores',
       x = NULL, y = NULL) +
  theme(axis.text = element_blank(),
        axis.title = element_blank())

# Convert to plotly for interactivity
ggplotly(plot_cpeat, tooltip = "text")
```

