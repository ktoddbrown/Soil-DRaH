---
title: "Carbon in Peat on EArth through Time"
author: "Vaasuki Marupaka"
date: "Summer 2024"
output: 
  html_document: 
    toc: true
    number_sections: true
    code_folding: show
---

The purpose of this document is to summarize the portions of the Carbon in Peat on EArth through Time (C-PEAT) Database that are relevant to data collections in the SoilDRaH project and walk through the data ingestion.
Here you will find links to documentation from the data provider, links to where you can access the data, a description of how the data was processed, and visuals for the collection relevant variables.

# What is C-PEAT

The version of the Carbon in Peat on EArth through Time (C-PEAT) Database described here is the public facing version of the PANGAEA repository (https://www.pangaea.de).

> **C-PEAT Database**
Peatlands played a key role in the global carbon cycle during the Holocene and previous interglacials. High-latitude and tropical peatlands have acted as net long-term atmospheric sinks for carbon dioxide (CO2). However, key uncertainties remain regarding many fundamental patterns and processes. C-PEAT aims to address these uncertainties.

Taken from [https://pastglobalchanges.org/science/wg/former/peat-carbon/intro](https://pastglobalchanges.org/science/wg/former/peat-carbon/intro) (Accessed 17-June-2024)

The annotations table draws heavily from the parameters list of the original data and also from the point of contact for C-PEAT collection of datasets (Dr. Julie Loisel). 

# Data processing

```{r setup, echo=TRUE, warning=FALSE, message=FALSE}

library(readr) # read in the csv tables
library(tibble) # use tibbles instead of a data frame
library(plyr) # transform a list into a data frame for the bind
library(dplyr) # work with data tables filters/joins/reframes
library(tidyr) # work with data table pivots
library(ggplot2) # make plots
library(stringr) # extract text from descriptions
library(knitr) # make prettier tables
library(kableExtra) #make tables scrollable
library(pangaear)

source("./R/readCPEAT.R", local = knitr::knit_global())

#locate the data locally
dataDir <- './temp'
dataAnnotations <- './data/CPEAT_annotations.csv'
```

```{r, echo=FALSE}
#Change this file to run locally for you
#dataDir  <- '~/Dropbox (UFL)/Research/Datasets/'
```

To read in the data base use the `readCPEAT` function.

```{r, warning=FALSE, message=FALSE}
CPEAT.ls <- readCPEAT(dataDir, 
                  annotationFilename = dataAnnotations, 
                  verbose = FALSE, #when first running, switch this to true 
                  format = 'long')


CPEAT.ls$long %>% # tital 615,152 obs with 9 columns
  select(doi, column_id, table_id, with_entry, row_number) %>% 
  slice_head(n=650) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```

```{r, eval=FALSE}
# TODO: Check data integrity of the long format
# Are all the rows identifiable? Can you match columns to their original rows and reconstruct the original tables?
```


There are two tables currently annotated in the C-PEAT data collection.
This represents the data directly relevant for layer-level soil carbon measurements including bulk density, organic carbon fraction, coarse fraction, depth of sample, sampling time, and sample location.
Below are the descriptions of data columns organized by table.

Please note that there is quite a bit more data in the C-PEAT and we welcome interested parties extending these annotations.

## STUDY TABLE 

```{r}
knitr::kable(CPEAT.ls$annotations %>%
               filter(of_type == 'description',
                      table_id == 'study') %>%
               select(column_id, of_variable, description = with_entry)) %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```

## CORE TABLE

```{r}
knitr::kable(CPEAT.ls$annotations %>%
               filter(of_type %in% c('description', 'unit'),
                      table_id == 'core') %>%
               select(column_id, of_variable, of_type, description = with_entry)) %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")

# test <- CPEAT.ls$annotations %>%
#                filter(of_type %in% c('description', 'unit'),
#                       table_id == 'core') %>%
#               select(column_id, of_variable, description = with_entry)
```

# Subseting for soil carbon

There is a lot of data here but not all of it is of interest for all purposes.
In this example we are interested in layer-level soil carbon related measurements and the geolocation.
Note that we are NOT checking any of the quality control flags here which you would want to do if using this for research purposes.

```{r}
# this function reads much better!

CPEAT_SOC <- CPEAT.ls$annotations %>% 
  #identify the variables that we are interested in
  filter(of_variable %in% c(
             "organic_carbon",
             "bulk_density",
             "layer_bottom" ,
             "layer_mid",
             "layer_top",
             "latitude",
             "longitude",
             "region",
             "elevation",
             "elevation_end",
             "elevation_start",
             "loss_on_ignition",
             "organic_matter",
             "layer_thickness",
             "total_carbon",
             "inorganic_carbon"),
         #limit these to tables since some of them are duplicates
         table_id %in% c('study', 'core'),
         #only look at those that map to the data as marked by '--'
         with_entry == '--') %>%
  #create a table for exclusive join
  select(table_id, column_name = column_id, of_variable, of_type) %>%
  #down select the data that matches the variables of interest above
  left_join(CPEAT.ls$long,
            by = join_by(table_id, column_name),
            relationship = "many-to-many")  %>%
  #only take the values, just incase there are any methods or units in the table
  filter(of_type == 'value', is.na(is_type) | is_type != 'description') %>%
  unique() %>%
  select(doi, column_name, with_entry, row_number) %>% 
  #make a single row ID
  #tidyr::unite(col = core_id, doi, sep = '::') %>%
  #group_by(doi, column_name) %>% 
  pivot_wider(names_from = column_name,
               values_from = with_entry) %>% # 62024 obs
  unique() %>% 
  dplyr::mutate(across(-doi, as.numeric))


CPEAT_SOC %>%
  slice_head(n=100) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")

```

## Casting values

Finally make sure that the numbers are actually numbers.

```{r}
# changing the structure of the column variables except doi
CPEAT_SOC <- CPEAT_SOC %>% 
  dplyr::mutate(across(-doi, as.numeric))

CPEAT_SOC %>%
  slice_head(n=100) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")

```

## Make small tables

Often this data is presented in three tables: a site level table with lat/lon and year, a layer level table with the depth and physiochemical information, and a meta table with information on the units and descriptions.
We given an example below of how this might be organized.

```{r}
# site level info
site.df <- CPEAT_SOC %>%
  select(doi, LATITUDE, LONGITUDE, LOCATION, ELEVATION, ELEVATION.START, ELEVATION.END) %>%
  filter(!is.na(LATITUDE)) %>% 
  unique() %>% 
  dplyr::mutate(across(-doi, as.numeric))

# metadata information for the datasets
meta.df <- CPEAT.ls$annotations %>%
  #identify the variables that we are interested in
  filter(of_variable %in% c('data_doi',
             'site', 'longtitude', 'latitude', 'location',
             'layer_center', 'layer_thickness', 'layer_midpoint', 'layer_depth_top', 'layer_depth_bottom',
             'bulk_density',
             'organic_matter_fraction', 'organic_matter_density', 'organic_carbon_density',
             'total_carbon', 'total_organic_carbon', 'loss_on _ignition', 'percent_carbon',
             'depth_loi', 'LOI', 'Total_inorganic_carbon', 'depth'),
         table_id %in% c('study', 'core'))

#just renaming the object to maintain uniformity in naming across datasets
layer.df <- CPEAT_SOC 
```

## Figures and Report

Below are some general figures that might be of interest if you are evaluating this database for use in a study.

# Plotting Layer histograms!

```{r}
ggplot(layer.df %>%
         pivot_longer(cols = where(is.numeric), values_drop_na = TRUE)) +
  geom_histogram(aes(x=value)) +
  facet_wrap(~name, scales='free')
```

# Bulk density vs Organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x=`DBD [g/cm**3]`, y = `C [%]`), alpha = 0.1)
```

# Bulk density vs Total organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x=`DBD [g/cm**3]`, y = `TOC [%]`), alpha = 0.1)
```

# Geolocations

```{r}
ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x=LONGITUDE, y = LATITUDE), alpha = .1, color = 'darkorange')+
  labs(x = NULL, y = NULL)
```

# Construct annotations

The annotations were partially constructed from the parameters table and other metadata provided with these data sets.
What follows is documentation code (not to be run) that shows how this was done.

```{r eval=FALSE}

#The code below is for documentation and developement purposes only and should not be executed when the code is generated.

stop('The code below here should not execute.')


# load in the original datasets from PANGAEA
CPEAT.original <- readCPEAT(dataDir, 
                  annotationFilename = dataAnnotations, 
                  verbose = TRUE,
                  format = 'original')


```

## Process data package

The CPEAT cores are complex data structures, each core contains a list with several primary and metadata elements.
In the following section we are going to disentangle the the structures and reallign truncated column names.

```{r eval=FALSE}
str(CPEAT.original[1])
```

```{r eval=FALSE}

#############################
###Shoestring all the data

  long.df <- plyr::ldply(CPEAT.original$orginal, .id = 'doi',
                             .fun = function(xx, verbose = FALSE) {
    
    if(verbose) print(paste('Processing - ', xx$doi))
                               
    ###################################
    #### Process layer data
                               
    if(length(names(xx$data)) > length(unique(names(xx$data)))){
      #warning(names(xx$data))
      warning(paste(xx$doi, "- Duplicate column names detected"))
      #TODO make this more elegant
      #right now pivoting column names has to be unique so adding a counter to the end
      names(xx$data) <- paste(names(xx$data), 1:length(names(xx$data)))
    }
                               
    colume_number <- tibble(column_name = names(xx$data),
                            column_number = 1:length(names(xx$data)))
    
    layerData <- xx$data %>%
      mutate(across(.cols = everything(), as.character)) %>%
      mutate(row_number = 1:n()) %>%
      pivot_longer(cols = -row_number, names_to = 'column_name',
                   values_to = 'with_entry',
                   values_drop_na = TRUE) %>%
      left_join(colume_number, by = join_by(column_name)) %>%
      mutate(table_name = 'data')
                             
    
    #####################################
    ### Process primary study information
    ### This information is stored in the first level list
    
    #list out all the possible names for the study info, be sure to update
    #...this list manually if the warning is thrown.
    primaryNames <- intersect(names(xx), c('parent_doi', 'doi', 'citation', 
                                           'url', 'path'))
    
    #There shouldn't be any names that we don't include above.
    if(any( ! (names(xx) %in% c(primaryNames, 'metadata', 'data')))){
      warning(paste('possible missing informatin at primary level for', xx$doi,
                    setdiff(names(xx), c(primaryNames, 'metadata', 'data'))))
    }
    
    ##############
    ###Meta data
    ###This information is stored under a list named 'metadata'
    
    #deal with the information in the metadata, again we name each possible
    #...list item here and if there are new names this needs to be updated.
    metaNames <- intersect(names(xx$metadata), c("citation", "related_to", "further_details",
                                                 "projects" , "coverage",
                                                 "abstract", "keywords",
                                                 "status",
                                                 "license", "size", "comment"))
    
    if(any( ! (names(xx$metadata) %in% c(metaNames, 'events', 'parameters')))){
      warning(paste('possible missing informatin at metadata level for', xx$doi))
    }
    
    #############
    ###Events
    ###This information is under the list 'metadata$events'
    
    #Pull in the study information from the 'events' item in the list
    #...again there should be no items that are not matching the manual array here
    eventsNames <- intersect(names(xx$metadata$events), 
                             c("LATITUDE", "LONGITUDE",
                               "ELEVATION", "ELEVATION START", "ELEVATION END",
                               "Penetration", "Recovery",
                               "LOCATION", "METHOD/DEVICE", 
                               "COMMENT"))
    
    #Take out the first item of the list which is actually the core name itself.
    if(any( ! (names(xx$metadata$events)[-1] %in%  
               c("LATITUDE", "LONGITUDE", "ELEVATION",
                 "ELEVATION START", "ELEVATION END", 
                 "Penetration","Recovery",
                 "LOCATION", "METHOD/DEVICE", "COMMENT")))){
      warning(paste('possible missing informatin at metadata-events level for', xx$doi))
    }
    
    
    #The core name is a special case where the information is in the name and not
    #...in the list values. Deal with that and append the events information.
    studyData <- as.data.frame(c(xx[primaryNames], # combining the primary study, metadata, and event data info into a single data frame
                                 xx$metadata[metaNames],
                                 list(core_name = names(xx$metadata$events)[1]), # this extracts and includes the core names (typically the first element of the 'events' sub-list)  as a separate entry column 
                                 xx$metadata$events[eventsNames])) %>%
      pivot_longer(cols = everything(), 
                   names_to = 'column_name',
                   values_to = 'with_entry')
                             
    
    ####### 
    ###Process parameter information
    
    if(xx$doi == "10.1594/PANGAEA.934274" &
       length(xx$metadata$parameters) > nrow(colume_number)){
      #A comment on the data age material got split between two column entries here
      #...combine them into the same entry
      xx$metadata$parameters[[5]] <- c(xx$metadata$parameters[[5]],
                                           xx$metadata$parameters[[6]])
      xx$metadata$parameters[6] <- NULL
    }
    
    # creating a description column to the column_number tibble for each parameter table by combining its elements into a single string.
    colume_number$description <- unlist(lapply(xx$metadata$parameters, 
                                               function(yy){
      return(paste(as.character(yy), collapse = ' '))
                                                 })) 
    
    layerData <- colume_number %>%
      pivot_longer(cols = description, 
                   names_to = 'is_type', 
                   values_to = 'with_entry') %>%
      mutate(table_name = 'data') %>%
      bind_rows(layerData) %>% 
      mutate(is_type = if_else(is.na(is_type), "value", is_type))
    
    # Convert all columns to character type and return the modified data
    return(bind_rows(studyData, layerData))
  }) 
  
```


### Begining QA/QC

```{r eval=FALSE}

##This table is incomplete and the text will need to be processed
##One strategy could be to use a fill with groupings
# ColumnNameCorrections <- tribble(~doi, ~column_index, ~column_name_new,
#   "10.1594/PANGAEA.934281", 10, 'Age unc [±] (Age, tephra-chronostratigraphy, calculated, 1 sigma)',
#  "10.1594/PANGAEA.934343",9, 'Age unc [±] (Age, tephra-chronostratigraphy, calculated, 1 sigma)',
#  "10.1594/PANGAEA.941094", 10, 'Age [a AD/CE] alt.',
#  "10.1594/PANGAEA.929068", 5, 'alt.',
#  "10.1594/PANGAEA.929068", 6, 'alt.',
#  "10.1594/PANGAEA.929068", 7, 'alt.',
#  "10.1594/PANGAEA.(929655)|(930133)", 2, 'Cal age [ka BP] (Median Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.(929655)|(930133)", 3, 'Cal age max [ka BP] (Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.(929655)|(930133)", 4, 'Cal age min [ka BP] (Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.(929655)|(930133)", 6, 'Cal age [ka BP] (Median Age, 14C calibrated, Bacon 2.2)',
#  "10.1594/PANGAEA.(929655)|(930133)", 7, 'Cal age max [ka BP] (Age, 14C calibrated, Bacon 2.2)', 
#  "10.1594/PANGAEA.(929655)|(930133)", 8, 'Cal age min [ka BP] (Age, 14C calibrated, Bacon 2.2)',
#  "10.1594/PANGAEA.930030", 2, 'Cal age [ka BP] (Median Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.930030", 3, 'Cal age max [ka BP] (Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.930030", 4, 'Cal age min [ka BP] (Age, 14C calibrated, OxCal 4.2.4)',
#  "10.1594/PANGAEA.930030", 5, 'Cal age [ka BP] (Median Age, 14C calibrated, Bacon 2.2)', 
#  "10.1594/PANGAEA.930030", 6, 'Cal age max [ka BP] (Age, 14C calibrated, Bacon 2.2)', 
#  "10.1594/PANGAEA.930030", 7, 'Cal age min [ka BP] (Age, 14C calibrated, Bacon 2.2)')
# 
# duplicateNames <- long.df %>%
#   filter(str_detect(column_name, pattern = '\\s\\d+$'),
#          is_type == 'description')

flags_ptn <- '((PI)|(METHOD)|(DEVICE)|(COMMENT)|$)'

studyAnnotation <- tribble(~column_name, ~description, ~of_variable, ~unit,
                               "core_name", "Core name", "core_name", NA_character_,
                               "parent_doi", "data package DOI", "data_doi",  NA_character_,
                               "doi",  "data package DOI", "data_doi",  NA_character_, #same as doi
                               "citation", "Citation", 'citation',  NA_character_,
                               "url", "download url", 'download_url',  NA_character_,
                               "path", "download path", 'data_local_filepath',  NA_character_,
                               "citation.1", "Citation", 'citation', NA_character_, #same as citation
                               "related_to", "Related citations", 'related_citation', NA_character_,
                               "projects", "Related project", 'project', NA_character_,
                               "coverage", "Bounded coverage", 'bounded_coverage', NA_character_,
                               "license", "Data license", 'data_license', NA_character_,
                               "size", "Observation size", 'observation_size', 'data point count',
                               "LATITUDE", "Latitude", 'latitude', 'decimal degree',
                               "LONGITUDE", "Longitude", 'longitude', 'decimal degree',
                               "LOCATION", "Adminstrative region", 'region', NA_character_,
                               "METHOD.DEVICE", "Sampling devise", 'sample_devise', NA_character_,
                               "COMMENT", "Data package comment", 'core_comment', NA_character_,
                               "further_details", "Citations", 'futher_citations', NA_character_,
                               "ELEVATION", "elevation", 'elevation', 'm',
                               "Recovery", "core length", 'core_length', 'cm',
                               "Penetration", "core length", 'core_length', 'cm',
                               "ELEVATION.START", "Start of elevation transect", 'elevation_start', 'm',
                               "ELEVATION.END", "End of elevation transect", 'elevation_end', 'm',
                               "abstract", "Abstract", 'abstract', NA_character_,
                               "keywords", "Keywords", 'keywords', NA_character_,
                               "status", "Curation level", 'cutation_level', NA_character_,
                               "comment", "Dataset Comment", 'core_comment', NA_character_) %>%
      mutate(table_id = 'study')

studyAnnotationSub1 <- studyAnnotation %>%
      pivot_longer(cols = c(description, unit), 
                   names_to = 'of_type',
                   values_to = 'with_entry',
                   values_drop_na = TRUE)
    
studyAnnotationSub2 <- studyAnnotation %>% 
      select(column_name, of_variable, table_id) %>%
      mutate(of_type = if_else(of_variable == 'core_comment', 'note', 'value'),
             with_entry = '--')

study_annotation.df <- bind_rows(studyAnnotationSub1, studyAnnotationSub2)  %>% # this stacks the above dataframes one over the other keeping all the rows of observations
  dplyr::rename(column_id = column_name) %>% # renaming the column names to match the data annotations template. 
  dplyr::arrange(table_id, of_variable, of_type) %>% # arranging the row observations based on the order given here. 
  dplyr::select(table_id, column_id, of_variable, of_type, with_entry)

long_extended.df <- long.df %>%
  #pull everything not associated with a specific row number
  filter(is.na(row_number)) %>% # selecting rows with row_number set to NA
  mutate(column_name = str_remove(column_name, pattern = '\\s\\d+$'),
         description = if_else(is.na(column_number), NA_character_, with_entry)) %>% # removing trailing digits
  mutate(new_name = trimws(str_remove(column_name, pattern = '(\\(|\\[).+'))) %>% # removing everything after and including '(' or '[' and trimming whitespaces
  mutate(unit = str_extract(column_name, pattern = '\\[.+\\]')) %>% # extract text within '[]' from column_name and associate it to unit column
  mutate(method = str_extract(column_name, pattern = '\\(.+\\)')) %>% # extract text within ( and ) from column_name and assign it to method.
  mutate(method = str_replace(method, pattern = 'Bacon 2\\.{4}', replacement = 'Bacon 2.2')) %>% # cleaning the given name and replacing it with correct name
  mutate(method = str_replace(method, pattern = 'OxCal 4\\.{4}', replacement = 'OxCal 4.2.4')) %>%
  mutate(PI = str_remove(str_extract(with_entry, pattern = 'PI.*$'), # Extract the substring starting with 'PI'
                         pattern = '((METHOD)|(DEVICE)|(COMMENT)).*$'),
          METHOD = str_remove(str_extract(with_entry, pattern = 'METHOD.*$'), # Extract the substring starting with 'METHOD'
                         pattern = '((PI)|(DEVICE)|(COMMENT)).*$'),
          DEVICE = str_remove(str_extract(with_entry, pattern = 'DEVICE.*$'), # Extract the substring starting with 'DEVICE'
                         pattern = '((METHOD)|(PI)|(COMMENT)).*$'),
          COMMENT = str_remove(str_extract(with_entry, pattern = 'COMMENT.*$'), # Extract the substring starting with 'COMMENT'
                         pattern = '((METHOD)|(DEVICE)|(PI)).*$')) %>%
  mutate(method = if_else(is.na(method), METHOD, paste(method, METHOD))) %>% # if method is NA, use METHOD; otherwise, concatenate method and METHOD.
  select(doi, table_id = table_name, column_id = new_name, column_name, column_number,
         unit, description, method, PI, device = DEVICE, comment = COMMENT) %>%
  unique() %>%
  pivot_longer(cols = all_of(c('unit', 'description', 'method', 'device', 'comment', 'PI')), 
               names_to = 'is_type', values_to = 'with_entry', values_drop_na = TRUE)  %>% # 8227 obs with 6 columns
  select(study_id = doi, table_id, column_id = column_name, of_variable = column_id, column_number, is_type, with_entry) %>% 
  write_csv('data/full_annotationsCPEAT.csv')

test <- long_extended.df %>%
  select(table_id, column_id) %>%
  mutate(across(everything(), trimws)) %>%
  unique() %>%
  arrange(table_id, column_id) %>%
  write_csv('temp/autoAnnotations.csv')

annotations <- read_csv('./data/CPEAT_annotations.csv') %>%
  mutate(new_name = trimws(str_remove(column_id, pattern = '(\\(|\\[).+'))) %>% # This effectively strips out any text within parentheses or brackets, as well as the parentheses or brackets themselves & trims any leading or trailing whitespace from the result of str_remove.
 # select(table_id, new_name, of_variable, of_type, with_entry) %>% 
  unique()
#Check the names matching
#setdiff(temp$new_name, long_extended.df$new_name)
```


```{r}
# trying to leverage the extended long dataframe above to construct annotations for core table
temp <- long_extended.df %>% 
  select(-column_number) # matches the column names of template annotations file; 24,641 obs 

temp2 <- long.df %>% 
  mutate(of_variable = trimws(str_remove(column_name, pattern = '(\\(|\\[).+'))) %>% 
  select(study_id = doi, table_id = table_name, column_id = column_name, of_variable, with_entry, row_number, column_number) %>% 
  bind_rows(temp) %>% 
  mutate(is_type = if_else(is.na(is_type), 'value', is_type),
         table_id = if_else(is.na(table_id), 'study', table_id)) # 639,793 obs
  #select(-c(row_number, column_number))
```

